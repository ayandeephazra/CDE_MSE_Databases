{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Bulk Modulus Generator.ipynb","provenance":[{"file_id":"1ocD12ql02AW3AON1p87rPo5NTY4y0iWW","timestamp":1628552748947},{"file_id":"https://github.com/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb","timestamp":1625004892726}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Cb4espuLKJiA"},"source":["##### Copyright 2020 The TensorFlow Hub Authors.\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"jM3hCI1UUzar"},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_NEJlxKKjyI"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/classify_text_with_bert\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","  <td>\n","    <a href=\"https://tfhub.dev/google/collections/bert/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"IZ6SNYq_tVVC"},"source":["# Classify text with BERT\n","\n","This tutorial contains complete code to fine-tune BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews.\n","In addition to training a model, you will learn how to preprocess text into an appropriate format.\n","\n","In this notebook, you will:\n","\n","- Load the IMDB dataset\n","- Load a BERT model from TensorFlow Hub\n","- Build your own model by combining BERT with a classifier\n","- Train your own model, fine-tuning BERT as part of that\n","- Save your model and use it to classify sentences\n","\n","If you're new to working with the IMDB dataset, please see [Basic text classification](https://www.tensorflow.org/tutorials/keras/text_classification) for more details."]},{"cell_type":"markdown","metadata":{"id":"2PHBpLPuQdmK"},"source":["## About BERT\n","\n","[BERT](https://arxiv.org/abs/1810.04805) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers. \n","\n","BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n"]},{"cell_type":"markdown","metadata":{"id":"SCjmX4zTCkRK"},"source":["## Setup\n"]},{"cell_type":"code","metadata":{"id":"q-YbjCkzw0yU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629227418230,"user_tz":300,"elapsed":91621,"user":{"displayName":"Ayandeep Hazra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqMdxE7KcehIBRI20OwPUswOKYzI2QFlIq93QhCg=s64","userId":"15981625870523489067"}},"outputId":"b3cc8619-d7e8-4c94-f0af-af4d52c56c02"},"source":["# A dependency of the preprocessing for BERT inputs\n","!pip install -q -U tensorflow-text"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 4.3 MB 4.2 MB/s \n","\u001b[K     |████████████████████████████████| 454.4 MB 9.6 kB/s \n","\u001b[K     |████████████████████████████████| 462 kB 38.7 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 42.4 MB/s \n","\u001b[K     |████████████████████████████████| 4.0 MB 44.3 MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5w_XlxN1IsRJ"},"source":["You will use the AdamW optimizer from [tensorflow/models](https://github.com/tensorflow/models)."]},{"cell_type":"code","metadata":{"id":"b-P1ZOA0FkVJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629227435180,"user_tz":300,"elapsed":16962,"user":{"displayName":"Ayandeep Hazra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqMdxE7KcehIBRI20OwPUswOKYzI2QFlIq93QhCg=s64","userId":"15981625870523489067"}},"outputId":"d8ad24e5-92d0-4468-bcb2-3a350d82b0ff"},"source":["!pip install -q tf-models-official"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.8 MB 4.3 MB/s \n","\u001b[K     |████████████████████████████████| 636 kB 41.8 MB/s \n","\u001b[K     |████████████████████████████████| 99 kB 8.0 MB/s \n","\u001b[K     |████████████████████████████████| 90 kB 8.9 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 49.6 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 38.3 MB/s \n","\u001b[K     |████████████████████████████████| 211 kB 50.0 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n","\u001b[K     |████████████████████████████████| 679 kB 37.7 MB/s \n","\u001b[K     |████████████████████████████████| 37.1 MB 47 kB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_XgTpm9ZxoN9"},"source":["import os\n","import shutil\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optimizer\n","\n","import matplotlib.pyplot as plt\n","\n","\n","tf.get_logger().setLevel('ERROR')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6MugfEgDRpY"},"source":["## Sentiment analysis\n","\n","This notebook trains a sentiment analysis model to classify movie reviews as *positive* or *negative*, based on the text of the review.\n","\n","You'll use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/)."]},{"cell_type":"markdown","metadata":{"id":"Vnvd4mrtPHHV"},"source":["### Download the IMDB dataset\n","\n","Let's download and extract the dataset, then explore the directory structure.\n"]},{"cell_type":"code","metadata":{"id":"pOdqCMoQDRJL"},"source":["#url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n","\n","#dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n","                                  #untar=True, cache_dir='.',\n","                                  #cache_subdir='')\n","\n","#dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n","\n","#train_dir = os.path.join(dataset_dir, 'train')\n","\n","# remove unused folders to make it easier to load the data\n","#remove_dir = os.path.join(train_dir, 'unsup')\n","#shutil.rmtree(remove_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lN9lWCYfPo7b"},"source":["Next, you will use the `text_dataset_from_directory` utility to create a labeled `tf.data.Dataset`.\n","\n","The IMDB dataset has already been divided into train and test, but it lacks a validation set. Let's create a validation set using an 80:20 split of the training data by using the `validation_split` argument below.\n","\n","Note:  When using the `validation_split` and `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation and training splits have no overlap."]},{"cell_type":"code","metadata":{"id":"6IwI_2bcIeX8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629227438374,"user_tz":300,"elapsed":1182,"user":{"displayName":"Ayandeep Hazra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqMdxE7KcehIBRI20OwPUswOKYzI2QFlIq93QhCg=s64","userId":"15981625870523489067"}},"outputId":"1a20671c-ea69-41c2-be24-a06e3b83cc6f"},"source":["import zipfile\n","zip_ref = zipfile.ZipFile(\"bulk_mod_combined.zip\", 'r')\n","zip_ref.extractall(\"/tmp\")\n","zip_ref.close()\n","\n","#unzip the relevant directories\n","!unzip 'combined_dataset_7.21.zip'\n","!unzip 'test.zip'\n","\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","batch_size = 32\n","seed = 42\n","\n","raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    'combined_dataset_7.21',\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset='training',\n","    seed=seed)\n","\n","class_names = raw_train_ds.class_names\n","train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    'combined_dataset_7.21',\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset='validation',\n","    seed=seed)\n","\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    'test',\n","    batch_size=batch_size)\n","\n","test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  combined_dataset_7.21.zip\n","   creating: combined_dataset_7.21/negative/\n","  inflating: combined_dataset_7.21/negative/001.txt  \n","  inflating: combined_dataset_7.21/negative/002.txt  \n","  inflating: combined_dataset_7.21/negative/003.txt  \n","  inflating: combined_dataset_7.21/negative/004.txt  \n","  inflating: combined_dataset_7.21/negative/005.txt  \n","  inflating: combined_dataset_7.21/negative/006.txt  \n","  inflating: combined_dataset_7.21/negative/007.txt  \n","  inflating: combined_dataset_7.21/negative/008.txt  \n","  inflating: combined_dataset_7.21/negative/009.txt  \n","  inflating: combined_dataset_7.21/negative/010.txt  \n","  inflating: combined_dataset_7.21/negative/011.txt  \n","  inflating: combined_dataset_7.21/negative/012.txt  \n","  inflating: combined_dataset_7.21/negative/013.txt  \n","  inflating: combined_dataset_7.21/negative/014.txt  \n","  inflating: combined_dataset_7.21/negative/015.txt  \n","  inflating: combined_dataset_7.21/negative/016.txt  \n","  inflating: combined_dataset_7.21/negative/017.txt  \n","  inflating: combined_dataset_7.21/negative/018.txt  \n","  inflating: combined_dataset_7.21/negative/019.txt  \n","  inflating: combined_dataset_7.21/negative/020.txt  \n","  inflating: combined_dataset_7.21/negative/021.txt  \n"," extracting: combined_dataset_7.21/negative/022.txt  \n","  inflating: combined_dataset_7.21/negative/023.txt  \n","  inflating: combined_dataset_7.21/negative/024.txt  \n","  inflating: combined_dataset_7.21/negative/025.txt  \n","  inflating: combined_dataset_7.21/negative/026.txt  \n","  inflating: combined_dataset_7.21/negative/027.txt  \n","  inflating: combined_dataset_7.21/negative/028.txt  \n","  inflating: combined_dataset_7.21/negative/029.txt  \n","  inflating: combined_dataset_7.21/negative/030.txt  \n","  inflating: combined_dataset_7.21/negative/031.txt  \n","  inflating: combined_dataset_7.21/negative/032.txt  \n"," extracting: combined_dataset_7.21/negative/033.txt  \n","  inflating: combined_dataset_7.21/negative/034.txt  \n","  inflating: combined_dataset_7.21/negative/035.txt  \n","  inflating: combined_dataset_7.21/negative/036.txt  \n","  inflating: combined_dataset_7.21/negative/037.txt  \n","  inflating: combined_dataset_7.21/negative/038.txt  \n","  inflating: combined_dataset_7.21/negative/039.txt  \n","  inflating: combined_dataset_7.21/negative/040.txt  \n"," extracting: combined_dataset_7.21/negative/041.txt  \n","  inflating: combined_dataset_7.21/negative/042.txt  \n","  inflating: combined_dataset_7.21/negative/043.txt  \n","  inflating: combined_dataset_7.21/negative/044.txt  \n","  inflating: combined_dataset_7.21/negative/045.txt  \n","  inflating: combined_dataset_7.21/negative/046.txt  \n","  inflating: combined_dataset_7.21/negative/047.txt  \n","  inflating: combined_dataset_7.21/negative/048.txt  \n","  inflating: combined_dataset_7.21/negative/049.txt  \n","  inflating: combined_dataset_7.21/negative/050.txt  \n","  inflating: combined_dataset_7.21/negative/051.txt  \n","  inflating: combined_dataset_7.21/negative/052.txt  \n","  inflating: combined_dataset_7.21/negative/053.txt  \n","  inflating: combined_dataset_7.21/negative/054.txt  \n","  inflating: combined_dataset_7.21/negative/055.txt  \n","  inflating: combined_dataset_7.21/negative/056.txt  \n","  inflating: combined_dataset_7.21/negative/057.txt  \n","  inflating: combined_dataset_7.21/negative/058.txt  \n","  inflating: combined_dataset_7.21/negative/059.txt  \n","  inflating: combined_dataset_7.21/negative/060.txt  \n","  inflating: combined_dataset_7.21/negative/061.txt  \n","  inflating: combined_dataset_7.21/negative/062.txt  \n","  inflating: combined_dataset_7.21/negative/063.txt  \n","  inflating: combined_dataset_7.21/negative/064.txt  \n","  inflating: combined_dataset_7.21/negative/065.txt  \n","  inflating: combined_dataset_7.21/negative/066.txt  \n","  inflating: combined_dataset_7.21/negative/067.txt  \n","  inflating: combined_dataset_7.21/negative/068.txt  \n","  inflating: combined_dataset_7.21/negative/069.txt  \n","  inflating: combined_dataset_7.21/negative/070.txt  \n","  inflating: combined_dataset_7.21/negative/071.txt  \n","  inflating: combined_dataset_7.21/negative/072.txt  \n","  inflating: combined_dataset_7.21/negative/073.txt  \n","  inflating: combined_dataset_7.21/negative/074.txt  \n","  inflating: combined_dataset_7.21/negative/075.txt  \n","  inflating: combined_dataset_7.21/negative/076.txt  \n","  inflating: combined_dataset_7.21/negative/077.txt  \n","  inflating: combined_dataset_7.21/negative/078.txt  \n","  inflating: combined_dataset_7.21/negative/079.txt  \n","  inflating: combined_dataset_7.21/negative/080.txt  \n","  inflating: combined_dataset_7.21/negative/081.txt  \n","  inflating: combined_dataset_7.21/negative/082.txt  \n","  inflating: combined_dataset_7.21/negative/083.txt  \n","  inflating: combined_dataset_7.21/negative/084.txt  \n","  inflating: combined_dataset_7.21/negative/085.txt  \n","  inflating: combined_dataset_7.21/negative/086.txt  \n","  inflating: combined_dataset_7.21/negative/087.txt  \n","  inflating: combined_dataset_7.21/negative/088.txt  \n","  inflating: combined_dataset_7.21/negative/089.txt  \n","  inflating: combined_dataset_7.21/negative/090.txt  \n","  inflating: combined_dataset_7.21/negative/091.txt  \n","  inflating: combined_dataset_7.21/negative/092.txt  \n","  inflating: combined_dataset_7.21/negative/093.txt  \n","  inflating: combined_dataset_7.21/negative/094.txt  \n","  inflating: combined_dataset_7.21/negative/095.txt  \n","  inflating: combined_dataset_7.21/negative/096.txt  \n","  inflating: combined_dataset_7.21/negative/097.txt  \n","  inflating: combined_dataset_7.21/negative/098.txt  \n","  inflating: combined_dataset_7.21/negative/099.txt  \n","  inflating: combined_dataset_7.21/negative/100.txt  \n","  inflating: combined_dataset_7.21/negative/101.txt  \n","  inflating: combined_dataset_7.21/negative/102.txt  \n","  inflating: combined_dataset_7.21/negative/103.txt  \n","  inflating: combined_dataset_7.21/negative/104.txt  \n","  inflating: combined_dataset_7.21/negative/105.txt  \n","  inflating: combined_dataset_7.21/negative/106.txt  \n","  inflating: combined_dataset_7.21/negative/107.txt  \n","  inflating: combined_dataset_7.21/negative/108.txt  \n","  inflating: combined_dataset_7.21/negative/109.txt  \n","  inflating: combined_dataset_7.21/negative/110.txt  \n","  inflating: combined_dataset_7.21/negative/111.txt  \n","  inflating: combined_dataset_7.21/negative/112.txt  \n","  inflating: combined_dataset_7.21/negative/113.txt  \n","  inflating: combined_dataset_7.21/negative/114.txt  \n","  inflating: combined_dataset_7.21/negative/115.txt  \n","  inflating: combined_dataset_7.21/negative/116.txt  \n","  inflating: combined_dataset_7.21/negative/117.txt  \n","  inflating: combined_dataset_7.21/negative/118.txt  \n","  inflating: combined_dataset_7.21/negative/119.txt  \n","  inflating: combined_dataset_7.21/negative/120.txt  \n","  inflating: combined_dataset_7.21/negative/121.txt  \n","  inflating: combined_dataset_7.21/negative/122.txt  \n","  inflating: combined_dataset_7.21/negative/123.txt  \n","  inflating: combined_dataset_7.21/negative/124.txt  \n","  inflating: combined_dataset_7.21/negative/125.txt  \n","  inflating: combined_dataset_7.21/negative/126.txt  \n","  inflating: combined_dataset_7.21/negative/127.txt  \n","  inflating: combined_dataset_7.21/negative/128.txt  \n","  inflating: combined_dataset_7.21/negative/129.txt  \n","  inflating: combined_dataset_7.21/negative/130.txt  \n","  inflating: combined_dataset_7.21/negative/131.txt  \n","  inflating: combined_dataset_7.21/negative/132.txt  \n","  inflating: combined_dataset_7.21/negative/133.txt  \n","  inflating: combined_dataset_7.21/negative/134.txt  \n","  inflating: combined_dataset_7.21/negative/135.txt  \n","  inflating: combined_dataset_7.21/negative/136.txt  \n","  inflating: combined_dataset_7.21/negative/137.txt  \n","  inflating: combined_dataset_7.21/negative/138.txt  \n","  inflating: combined_dataset_7.21/negative/139.txt  \n","  inflating: combined_dataset_7.21/negative/140.txt  \n","  inflating: combined_dataset_7.21/negative/141.txt  \n","  inflating: combined_dataset_7.21/negative/142.txt  \n","  inflating: combined_dataset_7.21/negative/143.txt  \n","  inflating: combined_dataset_7.21/negative/144.txt  \n","  inflating: combined_dataset_7.21/negative/145.txt  \n","  inflating: combined_dataset_7.21/negative/146.txt  \n","  inflating: combined_dataset_7.21/negative/147.txt  \n","  inflating: combined_dataset_7.21/negative/148.txt  \n","  inflating: combined_dataset_7.21/negative/149.txt  \n","  inflating: combined_dataset_7.21/negative/150.txt  \n","  inflating: combined_dataset_7.21/negative/151.txt  \n","  inflating: combined_dataset_7.21/negative/152.txt  \n","  inflating: combined_dataset_7.21/negative/153.txt  \n","  inflating: combined_dataset_7.21/negative/154.txt  \n","  inflating: combined_dataset_7.21/negative/155.txt  \n"," extracting: combined_dataset_7.21/negative/156.txt  \n","  inflating: combined_dataset_7.21/negative/157.txt  \n","  inflating: combined_dataset_7.21/negative/158.txt  \n","  inflating: combined_dataset_7.21/negative/159.txt  \n","  inflating: combined_dataset_7.21/negative/160.txt  \n","  inflating: combined_dataset_7.21/negative/161.txt  \n","  inflating: combined_dataset_7.21/negative/162.txt  \n","  inflating: combined_dataset_7.21/negative/163.txt  \n","  inflating: combined_dataset_7.21/negative/164.txt  \n","  inflating: combined_dataset_7.21/negative/165.txt  \n"," extracting: combined_dataset_7.21/negative/166.txt  \n","  inflating: combined_dataset_7.21/negative/167.txt  \n","  inflating: combined_dataset_7.21/negative/168.txt  \n","  inflating: combined_dataset_7.21/negative/169.txt  \n","  inflating: combined_dataset_7.21/negative/170.txt  \n","  inflating: combined_dataset_7.21/negative/171.txt  \n"," extracting: combined_dataset_7.21/negative/172.txt  \n","  inflating: combined_dataset_7.21/negative/173.txt  \n","  inflating: combined_dataset_7.21/negative/174.txt  \n","  inflating: combined_dataset_7.21/negative/175.txt  \n","  inflating: combined_dataset_7.21/negative/176.txt  \n","  inflating: combined_dataset_7.21/negative/177.txt  \n","  inflating: combined_dataset_7.21/negative/178.txt  \n","  inflating: combined_dataset_7.21/negative/179.txt  \n","  inflating: combined_dataset_7.21/negative/180.txt  \n","  inflating: combined_dataset_7.21/negative/181.txt  \n","  inflating: combined_dataset_7.21/negative/182.txt  \n","  inflating: combined_dataset_7.21/negative/183.txt  \n","  inflating: combined_dataset_7.21/negative/184.txt  \n","  inflating: combined_dataset_7.21/negative/185.txt  \n","  inflating: combined_dataset_7.21/negative/GP000.txt  \n","  inflating: combined_dataset_7.21/negative/GP001.txt  \n","  inflating: combined_dataset_7.21/negative/GP002.txt  \n","  inflating: combined_dataset_7.21/negative/GP003.txt  \n","  inflating: combined_dataset_7.21/negative/GP004.txt  \n","  inflating: combined_dataset_7.21/negative/GP005.txt  \n","  inflating: combined_dataset_7.21/negative/GP006.txt  \n","  inflating: combined_dataset_7.21/negative/GP007.txt  \n","  inflating: combined_dataset_7.21/negative/GP008.txt  \n","  inflating: combined_dataset_7.21/negative/GP009.txt  \n","  inflating: combined_dataset_7.21/negative/GP010.txt  \n","  inflating: combined_dataset_7.21/negative/GP011.txt  \n","  inflating: combined_dataset_7.21/negative/GP012.txt  \n","  inflating: combined_dataset_7.21/negative/GP013.txt  \n","  inflating: combined_dataset_7.21/negative/GP014.txt  \n","  inflating: combined_dataset_7.21/negative/GP015.txt  \n","  inflating: combined_dataset_7.21/negative/GP016.txt  \n","  inflating: combined_dataset_7.21/negative/GP017.txt  \n","  inflating: combined_dataset_7.21/negative/GP018.txt  \n","  inflating: combined_dataset_7.21/negative/GP019.txt  \n","  inflating: combined_dataset_7.21/negative/OP000.txt  \n","  inflating: combined_dataset_7.21/negative/OP001.txt  \n","  inflating: combined_dataset_7.21/negative/OP002.txt  \n","  inflating: combined_dataset_7.21/negative/OP003.txt  \n","  inflating: combined_dataset_7.21/negative/OP004.txt  \n","  inflating: combined_dataset_7.21/negative/OP005.txt  \n","  inflating: combined_dataset_7.21/negative/OP006.txt  \n","  inflating: combined_dataset_7.21/negative/OP007.txt  \n","  inflating: combined_dataset_7.21/negative/OP008.txt  \n","  inflating: combined_dataset_7.21/negative/OP009.txt  \n","  inflating: combined_dataset_7.21/negative/OP010.txt  \n","  inflating: combined_dataset_7.21/negative/OP011.txt  \n","  inflating: combined_dataset_7.21/negative/OP012.txt  \n","  inflating: combined_dataset_7.21/negative/OP013.txt  \n","  inflating: combined_dataset_7.21/negative/OP014.txt  \n","  inflating: combined_dataset_7.21/negative/OP015.txt  \n","  inflating: combined_dataset_7.21/negative/OP016.txt  \n"," extracting: combined_dataset_7.21/negative/OP017.txt  \n","  inflating: combined_dataset_7.21/negative/OP018.txt  \n","  inflating: combined_dataset_7.21/negative/OP019.txt  \n","  inflating: combined_dataset_7.21/negative/RN000.txt  \n"," extracting: combined_dataset_7.21/negative/RN001.txt  \n"," extracting: combined_dataset_7.21/negative/RN002.txt  \n","  inflating: combined_dataset_7.21/negative/RN003.txt  \n","  inflating: combined_dataset_7.21/negative/RN004.txt  \n","  inflating: combined_dataset_7.21/negative/RN005.txt  \n","  inflating: combined_dataset_7.21/negative/RN006.txt  \n","  inflating: combined_dataset_7.21/negative/RN007.txt  \n"," extracting: combined_dataset_7.21/negative/RN008.txt  \n","  inflating: combined_dataset_7.21/negative/RN009.txt  \n"," extracting: combined_dataset_7.21/negative/RN010.txt  \n","  inflating: combined_dataset_7.21/negative/RN011.txt  \n","  inflating: combined_dataset_7.21/negative/RN012.txt  \n","  inflating: combined_dataset_7.21/negative/RN013.txt  \n","  inflating: combined_dataset_7.21/negative/RN014.txt  \n","  inflating: combined_dataset_7.21/negative/RN015.txt  \n","  inflating: combined_dataset_7.21/negative/RN016.txt  \n","  inflating: combined_dataset_7.21/negative/RN017.txt  \n","  inflating: combined_dataset_7.21/negative/RN018.txt  \n","  inflating: combined_dataset_7.21/negative/RN019.txt  \n","   creating: combined_dataset_7.21/positive/\n","  inflating: combined_dataset_7.21/positive/001.txt  \n","  inflating: combined_dataset_7.21/positive/002.txt  \n","  inflating: combined_dataset_7.21/positive/003.txt  \n","  inflating: combined_dataset_7.21/positive/004.txt  \n","  inflating: combined_dataset_7.21/positive/005.txt  \n","  inflating: combined_dataset_7.21/positive/006.txt  \n","  inflating: combined_dataset_7.21/positive/007.txt  \n","  inflating: combined_dataset_7.21/positive/008.txt  \n"," extracting: combined_dataset_7.21/positive/009.txt  \n","  inflating: combined_dataset_7.21/positive/010.txt  \n","  inflating: combined_dataset_7.21/positive/011.txt  \n"," extracting: combined_dataset_7.21/positive/012.txt  \n","  inflating: combined_dataset_7.21/positive/013.txt  \n","  inflating: combined_dataset_7.21/positive/014.txt  \n","  inflating: combined_dataset_7.21/positive/015.txt  \n"," extracting: combined_dataset_7.21/positive/016.txt  \n","  inflating: combined_dataset_7.21/positive/017.txt  \n","  inflating: combined_dataset_7.21/positive/018.txt  \n"," extracting: combined_dataset_7.21/positive/019.txt  \n","  inflating: combined_dataset_7.21/positive/020.txt  \n","  inflating: combined_dataset_7.21/positive/021.txt  \n","  inflating: combined_dataset_7.21/positive/022.txt  \n","  inflating: combined_dataset_7.21/positive/023.txt  \n","  inflating: combined_dataset_7.21/positive/024.txt  \n","  inflating: combined_dataset_7.21/positive/025.txt  \n","  inflating: combined_dataset_7.21/positive/026.txt  \n","  inflating: combined_dataset_7.21/positive/027.txt  \n","  inflating: combined_dataset_7.21/positive/028.txt  \n","  inflating: combined_dataset_7.21/positive/029.txt  \n","  inflating: combined_dataset_7.21/positive/030.txt  \n","  inflating: combined_dataset_7.21/positive/031.txt  \n","  inflating: combined_dataset_7.21/positive/032.txt  \n","  inflating: combined_dataset_7.21/positive/033.txt  \n","  inflating: combined_dataset_7.21/positive/034.txt  \n","  inflating: combined_dataset_7.21/positive/035.txt  \n","  inflating: combined_dataset_7.21/positive/036.txt  \n","  inflating: combined_dataset_7.21/positive/037.txt  \n","  inflating: combined_dataset_7.21/positive/038.txt  \n","  inflating: combined_dataset_7.21/positive/039.txt  \n","  inflating: combined_dataset_7.21/positive/040.txt  \n"," extracting: combined_dataset_7.21/positive/041.txt  \n","  inflating: combined_dataset_7.21/positive/042.txt  \n","  inflating: combined_dataset_7.21/positive/043.txt  \n","  inflating: combined_dataset_7.21/positive/044.txt  \n","  inflating: combined_dataset_7.21/positive/045.txt  \n","  inflating: combined_dataset_7.21/positive/046.txt  \n","  inflating: combined_dataset_7.21/positive/047.txt  \n","  inflating: combined_dataset_7.21/positive/048.txt  \n","  inflating: combined_dataset_7.21/positive/049.txt  \n","  inflating: combined_dataset_7.21/positive/050.txt  \n","  inflating: combined_dataset_7.21/positive/051.txt  \n","  inflating: combined_dataset_7.21/positive/052.txt  \n","  inflating: combined_dataset_7.21/positive/053.txt  \n","  inflating: combined_dataset_7.21/positive/054.txt  \n"," extracting: combined_dataset_7.21/positive/055.txt  \n","  inflating: combined_dataset_7.21/positive/056.txt  \n","  inflating: combined_dataset_7.21/positive/057.txt  \n","  inflating: combined_dataset_7.21/positive/058.txt  \n","  inflating: combined_dataset_7.21/positive/059.txt  \n","  inflating: combined_dataset_7.21/positive/060.txt  \n","  inflating: combined_dataset_7.21/positive/061.txt  \n","  inflating: combined_dataset_7.21/positive/062.txt  \n","  inflating: combined_dataset_7.21/positive/063.txt  \n","  inflating: combined_dataset_7.21/positive/064.txt  \n","  inflating: combined_dataset_7.21/positive/065.txt  \n","  inflating: combined_dataset_7.21/positive/066.txt  \n","  inflating: combined_dataset_7.21/positive/067.txt  \n","  inflating: combined_dataset_7.21/positive/068.txt  \n","  inflating: combined_dataset_7.21/positive/069.txt  \n","  inflating: combined_dataset_7.21/positive/070.txt  \n","  inflating: combined_dataset_7.21/positive/071.txt  \n","  inflating: combined_dataset_7.21/positive/072.txt  \n","  inflating: combined_dataset_7.21/positive/073.txt  \n","  inflating: combined_dataset_7.21/positive/074.txt  \n","  inflating: combined_dataset_7.21/positive/075.txt  \n","  inflating: combined_dataset_7.21/positive/076.txt  \n"," extracting: combined_dataset_7.21/positive/077.txt  \n","  inflating: combined_dataset_7.21/positive/078.txt  \n","  inflating: combined_dataset_7.21/positive/079.txt  \n","  inflating: combined_dataset_7.21/positive/080.txt  \n","  inflating: combined_dataset_7.21/positive/081.txt  \n","  inflating: combined_dataset_7.21/positive/082.txt  \n","  inflating: combined_dataset_7.21/positive/083.txt  \n","  inflating: combined_dataset_7.21/positive/084.txt  \n"," extracting: combined_dataset_7.21/positive/085.txt  \n","  inflating: combined_dataset_7.21/positive/086.txt  \n","  inflating: combined_dataset_7.21/positive/087.txt  \n","  inflating: combined_dataset_7.21/positive/088.txt  \n","  inflating: combined_dataset_7.21/positive/089.txt  \n","  inflating: combined_dataset_7.21/positive/090.txt  \n","  inflating: combined_dataset_7.21/positive/091.txt  \n","  inflating: combined_dataset_7.21/positive/092.txt  \n","  inflating: combined_dataset_7.21/positive/093.txt  \n","  inflating: combined_dataset_7.21/positive/094.txt  \n","  inflating: combined_dataset_7.21/positive/095.txt  \n","  inflating: combined_dataset_7.21/positive/096.txt  \n","  inflating: combined_dataset_7.21/positive/097.txt  \n","  inflating: combined_dataset_7.21/positive/098.txt  \n","  inflating: combined_dataset_7.21/positive/099.txt  \n","  inflating: combined_dataset_7.21/positive/100.txt  \n","  inflating: combined_dataset_7.21/positive/101.txt  \n","  inflating: combined_dataset_7.21/positive/102.txt  \n"," extracting: combined_dataset_7.21/positive/103.txt  \n","  inflating: combined_dataset_7.21/positive/104.txt  \n","  inflating: combined_dataset_7.21/positive/105.txt  \n","  inflating: combined_dataset_7.21/positive/106.txt  \n","  inflating: combined_dataset_7.21/positive/107.txt  \n"," extracting: combined_dataset_7.21/positive/108.txt  \n","  inflating: combined_dataset_7.21/positive/109.txt  \n","  inflating: combined_dataset_7.21/positive/110.txt  \n","  inflating: combined_dataset_7.21/positive/111.txt  \n","  inflating: combined_dataset_7.21/positive/112.txt  \n","  inflating: combined_dataset_7.21/positive/113.txt  \n","  inflating: combined_dataset_7.21/positive/114.txt  \n","  inflating: combined_dataset_7.21/positive/115.txt  \n","  inflating: combined_dataset_7.21/positive/116.txt  \n","  inflating: combined_dataset_7.21/positive/117.txt  \n","  inflating: combined_dataset_7.21/positive/118.txt  \n","  inflating: combined_dataset_7.21/positive/119.txt  \n","  inflating: combined_dataset_7.21/positive/120.txt  \n","Archive:  test.zip\n","   creating: test/negative/\n","  inflating: test/negative/testneg1 (1).txt  \n","  inflating: test/negative/testneg2.txt  \n","   creating: test/positive/\n","  inflating: test/positive/test1.txt  \n","  inflating: test/positive/test2.txt  \n","Found 365 files belonging to 2 classes.\n","Using 292 files for training.\n","Found 365 files belonging to 2 classes.\n","Using 73 files for validation.\n","Found 4 files belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HGm10A5HRGXp"},"source":["Let's take a look at a few reviews."]},{"cell_type":"code","metadata":{"id":"JuxDkcvVIoev","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629227438375,"user_tz":300,"elapsed":4,"user":{"displayName":"Ayandeep Hazra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqMdxE7KcehIBRI20OwPUswOKYzI2QFlIq93QhCg=s64","userId":"15981625870523489067"}},"outputId":"dff3db63-1ea9-4117-8068-7c70c8326bea"},"source":["for text_batch, label_batch in train_ds.take(1):\n","  for i in range(30):\n","    print(f'Review: {text_batch.numpy()[i]}')\n","    label = label_batch.numpy()[i]\n","    print(f'Label : {label} ({class_names[label]})')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Review: b'A third order Birch\\xe2\\x80\\x93Murnaghan fit of the unit cell volume as a function of\\xc2\\xa0pressure\\xc2\\xa0for all experimental points, yielded a zero\\xc2\\xa0pressure\\xc2\\xa0bulk modulus\\xc2\\xa0K0=142(7)\\xe2\\x80\\x82GPa\\xe2\\x80\\x82GPa, and its\\xc2\\xa0pressure\\xc2\\xa0derivative\\xc2\\xa0K\\xe2\\x80\\xb20=3.3(0.2)\\xc2\\xa0for the\\xc2\\xa0high-pressure\\xc2\\xa0tetragonal phase of\\xc2\\xa0TiH2\\xc2\\xa0and with\\xc2\\xa0K\\xe2\\x80\\xb20\\xc2\\xa0held at four,\\xc2\\xa0K0=130(5)\\xe2\\x80\\x82GPa.\\n'\n","Label : 1 (positive)\n","Review: b'For the specific volume of 59.65\\xe2\\x80\\x89\\xc2\\xb1\\xe2\\x80\\x890.07 \\xc3\\x853 the average measured isothermal bulk modulus of scandia from the present study and recent literature results is 188\\xe2\\x80\\x89\\xc2\\xb1\\xe2\\x80\\x8910 GPa.\\n'\n","Label : 1 (positive)\n","Review: b'The\\xc2\\xa0bulk modulus\\xc2\\xa0of the\\xc2\\xa0beryllium\\xc2\\xa0tetrahedron in BeO is 210 GPa, identical to that of the crystal, and close to the value observed for\\xc2\\xa0beryllium\\xc2\\xa0tetrahedra in other\\xc2\\xa0beryllium\\xc2\\xa0minerals.\\n'\n","Label : 1 (positive)\n","Review: b'A bulk modulus\\xc2\\xa0B0\\xc2\\xa0= 118 GPa and its pressure derivative\\xc2\\xa0B\\xe2\\x80\\xb20\\xc2\\xa0= 6.6 were calculated by fitting the\\xc2\\xa0V(P) data to the Birch and Murnaghan equations.\\xc2\\xa0\\n'\n","Label : 1 (positive)\n","Review: b'It is this property which entices automobile manufacturers to replace denser materials, not only steels, cast irons and copper base alloys but even aluminium alloys by magnesium based alloys.\\n'\n","Label : 0 (negative)\n","Review: b'The RXRD data obtained at \\xcf\\x88 = 54.7 \\xc2\\xb0 yield a bulk modulus K 0 = 282 \\xc2\\xb1 9 GPa.\\n'\n","Label : 1 (positive)\n","Review: b'The cubic fluorite phase has a bulk modulus\\xc2\\xa0B0=230\\xc2\\xb110 GPa with an assumed pressure derivative of\\xc2\\xa0B\\xe2\\x80\\x990=4.00.\\n'\n","Label : 1 (positive)\n","Review: b'The out-of-plane strain has been applied to tune the bandgap and its electrical properties.\\n'\n","Label : 0 (negative)\n","Review: b'Based upon simulation results and reanalysis of experimental data, the commonly accepted value of the initial isothermal\\xc2\\xa0bulk modulus\\xc2\\xa0for \\xce\\xb2-HMX should be revised from a value of\\xc2\\xa0\\xe2\\x88\\xbc12.4\\xe2\\x80\\x9313.5\\xe2\\x80\\x89GPa\\xe2\\x88\\xbc12.4\\xe2\\x80\\x9313.5\\xe2\\x80\\x89GPa\\xc2\\xa0to\\xc2\\xa0\\xe2\\x88\\xbc15\\xe2\\x80\\x9316\\xe2\\x80\\x89GPa.\\n'\n","Label : 1 (positive)\n","Review: b'A new approach has been developed to yield both the particle density and particle mass (or size) of colloidal materials using a sedimentation field-flow fractionation system. The method involves varying the density of the carrier fluid, in the present case using aqueous sucrose solutions.\\n'\n","Label : 0 (negative)\n","Review: b'The properties of graphene-like BC6N semiconductor are studied using density functional theory taking into account the attractive interaction between B and N atoms\\n'\n","Label : 0 (negative)\n","Review: b'Yann Martel is the author of the famous book \"Life of Pi\".'\n","Label : 0 (negative)\n","Review: b'The ternary potentials accurately capture the heat of mixing and structural properties associated with solid solution alloys of palladium-silver\\n'\n","Label : 0 (negative)\n","Review: b'So far, there has been no report on superconducting devices based on CoFeB.\\n'\n","Label : 0 (negative)\n","Review: b'A bulk modulus B 0= 118 GPa and its pressure derivative B\\xe2\\x80\\xb2 0= 6.6 were calculated by fitting the equation of state.\\n'\n","Label : 1 (positive)\n","Review: b'The zero-pressure bulk modulus of SiC2 is one of the highest known in crystalline materials at 230 GPa.\\n'\n","Label : 1 (positive)\n","Review: b'The results of theoretical calculations of the intensity of scattering I(k,\\xcf\\x89) for amorphous Ni33Zr67 are in a good agreement with the results of computer simulation as well as with the experimental data on inelastic X-ray scattering.\\n'\n","Label : 0 (negative)\n","Review: b'We can conclude that the largest values of bulk modulus 57.9173, 55.998 and 52.988\\xe2\\x80\\xafGPa obtained for ZnSe0.75Te0.25, ZnSe0.50Te0.50, and ZnSe0.25Te0.75 structures simply relatively less compressible compounds.\\n'\n","Label : 1 (positive)\n","Review: b\"For Indiana limestone, best fit values were drained bulk modulus, 21.2 GPa; the undrained bulk modulus, 31.7 GPa; drained Poisson's ratio, 0.26; undrained Poisson's ratio, 0.33; and pore pressure buildup coefficient, 0.47 at 20\\xe2\\x80\\x9335 MPa effective stress.\\n\"\n","Label : 1 (positive)\n","Review: b'With the Voigt average relation, the bulk modulus of phase F was estimated to be 130 GPa using the bulk modulus values 160 GPa for periclase and 145 GPa for superhydrous phase B.\\n'\n","Label : 1 (positive)\n","Review: b'The electrical and structural properties of Co40Fe40B20 (CoFeB) alloy are tunable with thermal annealing\\n'\n","Label : 0 (negative)\n","Review: b'Infiltration from ring infiltrometers of different radii was measured into four soil materials contained in laboratory tanks. \\n'\n","Label : 0 (negative)\n","Review: b'For BaCe1\\xe2\\x88\\x92xYbxO3\\xe2\\x88\\x92\\xce\\xb4 at low doping levels (0 \\xe2\\xa9\\xbd x \\xe2\\xa9\\xbd 0.05) a noticeable effect on the temperature shift of the polymorphic transformations was reported.\\n'\n","Label : 0 (negative)\n","Review: b'Three different polymers with a macroscopic Young\\xe2\\x80\\x99s modulus of 0.6\\xe2\\x80\\x930.7 GPa (polyurethanes) and 2.7 GPa (polystyrene) are analyzed using these new modes.'\n","Label : 0 (negative)\n","Review: b'In this preliminary report, we discuss the correlation between B\\xe2\\x80\\x93H atomistic vibrations in [BH4]\\xe2\\x88\\x92-anion and melting temperatures of MBH4 (), on the basis of the experimental results combined the Raman spectroscopy and differential scanning calorimetry. Both the B\\xe2\\x80\\x93H stretching modes \\xce\\xbd1 and B\\xe2\\x80\\x93H bending modes \\xce\\xbd2, decrease in the order of LiBH4>NaBH4>KBH4, except for \\xce\\xbd1 of LiBH4. \\n'\n","Label : 0 (negative)\n","Review: b'Charles Darwin popularized the idea of evolution, suggesting we evolved from apes.'\n","Label : 0 (negative)\n","Review: b'Hence, CaSiO3\\xc2\\xa0must be considered an invisible component, in terms of density and bulk modulus constraints, in the lower mantle.\\xc2\\xa0\\n'\n","Label : 0 (negative)\n","Review: b'The band-gap has been calculated within the MBJLDA approximation\\n'\n","Label : 0 (negative)\n","Review: b'Nanoindentation tests also showed that the hardness and elastic modulus increase from \\xe2\\x88\\xbc6.45 GPa and \\xe2\\x88\\xbc113.0 GPa (as-cast state) to \\xe2\\x88\\xbc7.27 GPa and \\xe2\\x88\\xbc130.9 GPa after annealed for 20 min, respectively.'\n","Label : 0 (negative)\n","Review: b'Ringer\\xe2\\x80\\x99s solution, saline solution, and distilled water had the highest surface tension values, whereas those of NaOCl (2.5% and 5%) and 17% EDTA were relatively low. \\n'\n","Label : 0 (negative)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dX8FtlpGJRE6"},"source":["## Loading models from TensorFlow Hub\n","\n","Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n","\n","  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n","  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n","  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n","  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n","  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n","  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n","\n","The model documentation on TensorFlow Hub has more details and references to the\n","research literature. Follow the links above, or click on the [`tfhub.dev`](http://tfhub.dev) URL\n","printed after the next cell execution.\n","\n","The suggestion is to start with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose\n","one of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n","\n","Aside from the models available below, there are [multiple versions](https://tfhub.dev/google/collections/transformer_encoders_text/1) of the models that are larger and can yield even better accuracy, but they are too big to be fine-tuned on a single GPU. You will be able to do that on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n","\n","You'll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub."]},{"cell_type":"code","metadata":{"id":"y8_ctG55-uTX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629227438619,"user_tz":300,"elapsed":247,"user":{"displayName":"Ayandeep Hazra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqMdxE7KcehIBRI20OwPUswOKYzI2QFlIq93QhCg=s64","userId":"15981625870523489067"}},"outputId":"547d9335-5dc2-46fd-b199-2991db0bec81"},"source":["#@title Choose a BERT model to fine-tune\n","\n","bert_model_name = \"bert_en_uncased_L-12_H-768_A-12\" #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"] {allow-input: true}\n","\n","map_name_to_handle = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_base/2',\n","    'electra_small':\n","        'https://tfhub.dev/google/electra_small/2',\n","    'electra_base':\n","        'https://tfhub.dev/google/electra_base/2',\n","    'experts_pubmed':\n","        'https://tfhub.dev/google/experts/bert/pubmed/2',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n","        'SciBERT':'https://huggingface.co/allenai/scibert_scivocab_uncased',\n","}\n","\n","map_model_to_preprocess = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n","    'electra_small':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'electra_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_pubmed':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","        'SciBERT':'https://huggingface.co/allenai/scibert_scivocab_uncased',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7WrcxxTRDdHi"},"source":["## The preprocessing model\n","\n","Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n","\n","The preprocessing model must be the one referenced by the documentation of the BERT model, which you can read at the URL printed above. For BERT models from the drop-down above, the preprocessing model is selected automatically.\n","\n","Note: You will load the preprocessing model into a [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model."]},{"cell_type":"code","metadata":{"id":"avucOJVVMsl9"},"source":["#tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","\n","#tokenizer.save_pretrained('models')\n","\n","#model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","\n","#model.save_pretrained('models')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IwdaxZ0lhaW4"},"source":["\n","checkpoint = tf.train.Checkpoint()\n","save_path = checkpoint.save('models/')\n","checkpoint.restore(save_path)\n","#checkpoint.save_pretrained('models')\n","\n","export_dir = 'export_dir' \n","trained_checkpoint_prefix = 'models/model.ckpt'\n","graph = tf.Graph()\n","#loader = tf.train.import_meta_graph(trained_checkpoint_prefix + \".meta\" )\n","#sess = tf.Session()\n","#loader.restore(sess,trained_checkpoint_prefix)\n","#builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n","#builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.TRAINING, tf.saved_model.tag_constants.SERVING], strip_default_attrs=True)\n","#builder.save()\n"]},{"cell_type":"code","metadata":{"id":"0SQi-jWd_jzq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629230089924,"user_tz":300,"elapsed":69588,"user":{"displayName":"Ayandeep Hazra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqMdxE7KcehIBRI20OwPUswOKYzI2QFlIq93QhCg=s64","userId":"15981625870523489067"}},"outputId":"ef387523-0bc8-46d2-ed2e-465b8b82f0b6"},"source":["# https://stackoverflow.com/questions/56766639/how-to-convert-ckpt-to-pb # v 1.7 of tf\n","\n","!pip install transformers\n","import transformers\n","from transformers import *\n","\n","bert_model = transformers.TFBertModel.from_pretrained('allenai/scibert_scivocab_uncased', from_pt = True)\n","print(type(bert_model))\n","bert_model.save_pretrained('l')\n","tf.saved_model.save(bert_model, 'SciBERT')\n","\n","\n","bert_preprocess_model = hub.KerasLayer('SciBERT')\n","print(\"here is the type\",type(bert_preprocess_model).__name__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["here is the type KerasLayer\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x4naBiEE_cZX"},"source":["Let's try the preprocessing model on some text and see the output:"]},{"cell_type":"code","metadata":{"id":"r9-zCzJpnuwS","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1629227532896,"user_tz":300,"elapsed":308,"user":{"displayName":"Ayandeep Hazra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqMdxE7KcehIBRI20OwPUswOKYzI2QFlIq93QhCg=s64","userId":"15981625870523489067"}},"outputId":"d7dbcb41-97a4-498e-cc07-08d468f7ab50"},"source":["text_test = ['this is such an amazing movie!']\n","text_preprocessed = bert_preprocess_model(text_test)\n","\n","print(f'Keys       : {list(text_preprocessed.keys())}')\n","print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n","print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n","print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n","print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-f4f0bd8f7d5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'this is such an amazing movie!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_preprocess_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Keys       : {list(text_preprocessed.keys())}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Shape      : {text_preprocessed[\"input_word_ids\"].shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    237\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[1;32m    238\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                                      lambda: f(training=False))\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[1;32m    238\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                                      lambda: f(training=False))\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mrestored_function_body\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m         .format(_pretty_format_positional(args), kwargs,\n\u001b[1;32m    290\u001b[0m                 \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcrete_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \"\\n\\n\".join(signature_descriptions)))\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[0mconcrete_function_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Could not find matching function to call loaded from the SavedModel. Got:\n  Positional arguments (10 total):\n    * ['this is such an amazing movie!']\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * False\n  Keyword arguments: {}\n\nExpected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (10 total):\n    * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * False\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (10 total):\n    * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * True\n  Keyword arguments: {}"]}]},{"cell_type":"markdown","metadata":{"id":"EqL7ihkN_862"},"source":["As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (`input_words_id`, `input_mask` and `input_type_ids`).\n","\n","Some other important points:\n","- The input is truncated to 128 tokens. The number of tokens can be customized, and you can see more details on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n","- The `input_type_ids` only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.\n","\n","Since this text preprocessor is a TensorFlow model, It can be included in your model directly."]},{"cell_type":"markdown","metadata":{"id":"DKnLPSEmtp9i"},"source":["## Using the BERT model\n","\n","Before putting BERT into your own model, let's take a look at its outputs. You will load it from TF Hub and see the returned values."]},{"cell_type":"code","metadata":{"id":"tXxYpK8ixL34"},"source":["bert_model = hub.KerasLayer(tfhub_handle_encoder)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OoF9mebuSZc"},"source":["bert_results = bert_model(text_preprocessed)\n","\n","print(f'Loaded BERT: {tfhub_handle_encoder}')\n","print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n","print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n","print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n","print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sm61jDrezAll"},"source":["The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:\n","\n","- `pooled_output` represents each input sequence as a whole. The shape is `[batch_size, H]`. You can think of this as an embedding for the entire movie review.\n","- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. You can think of this as a contextual embedding for every token in the movie review.\n","- `encoder_outputs` are the intermediate activations of the `L` Transformer blocks. `outputs[\"encoder_outputs\"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the i-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`.\n","\n","For the fine-tuning you are going to use the `pooled_output` array."]},{"cell_type":"markdown","metadata":{"id":"pDNKfAXbDnJH"},"source":["## Define your model\n","\n","You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.\n","\n","Note: for more information about the base model's input and output you can follow the model's URL for documentation. Here specifically, you don't need to worry about it because the preprocessing model will take care of that for you.\n"]},{"cell_type":"code","metadata":{"id":"LD0KT4KuD-Ou"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aksj743St9ga"},"source":["def build_classifier_model():\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","  preprocessing_layer = hub.KerasLayer('SciBERT', name='preprocessing')\n","  encoder_inputs = preprocessing_layer(text_input)\n","  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  net = outputs['pooled_output']\n","  net = tf.keras.layers.Dropout(0.05)(net)\n","  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n","  return tf.keras.Model(text_input, net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zs4yhFraBuGQ"},"source":["Let's check that the model runs with the output of the preprocessing model."]},{"cell_type":"code","metadata":{"id":"mGMF8AZcB2Zy"},"source":["classifier_model = build_classifier_model()\n","bert_raw_result = classifier_model(tf.constant(text_test))\n","print(tf.sigmoid(bert_raw_result))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZTUzNV2JE2G3"},"source":["The output is meaningless, of course, because the model has not been trained yet.\n","\n","Let's take a look at the model's structure."]},{"cell_type":"code","metadata":{"id":"0EmzyHZXKIpm"},"source":["tf.keras.utils.plot_model(classifier_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WbUWoZMwc302"},"source":["## Model training\n","\n","You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier."]},{"cell_type":"markdown","metadata":{"id":"WpJ3xcwDT56v"},"source":["### Loss function\n","\n","Since this is a binary classification problem and the model outputs a probability (a single-unit layer), you'll use `losses.BinaryCrossentropy` loss function.\n"]},{"cell_type":"code","metadata":{"id":"OWPOZE-L3AgE"},"source":["loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","metrics = tf.metrics.BinaryAccuracy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77psrpfzbxtp"},"source":["### Optimizer\n","\n","For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n","\n","For the learning rate (`init_lr`), you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."]},{"cell_type":"code","metadata":{"id":"P9eP2y9dbw32"},"source":["epochs = 5\n","steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","init_lr = 10e-5\n","optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SqlarlpC_v0g"},"source":["### Loading the BERT model and training\n","\n","Using the `classifier_model` you created earlier, you can compile the model with the loss, metric and optimizer."]},{"cell_type":"code","metadata":{"id":"-7GPDhR98jsD"},"source":["classifier_model.compile(optimizer=optimizer,\n","                         loss=loss,\n","                         metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CpBuV5j2cS_b"},"source":["Note: training time will vary depending on the complexity of the BERT model you have selected."]},{"cell_type":"code","metadata":{"id":"HtfDFAnN_Neu"},"source":["print(f'Training model with {tfhub_handle_encoder}')\n","history = classifier_model.fit(x=train_ds, validation_data=val_ds, epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uBthMlTSV8kn"},"source":["### Evaluate the model\n","\n","Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."]},{"cell_type":"code","metadata":{"id":"slqB-urBV9sP"},"source":["loss, accuracy = classifier_model.evaluate(test_ds)\n","\n","print(f'Loss: {loss}')\n","print(f'Accuracy: {accuracy}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uttWpgmSfzq9"},"source":["### Plot the accuracy and loss over time\n","\n","Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"]},{"cell_type":"code","metadata":{"id":"fiythcODf0xo"},"source":["history_dict = history.history\n","print(history_dict.keys())\n","\n","acc = history_dict['binary_accuracy']\n","val_acc = history_dict['val_binary_accuracy']\n","loss = history_dict['loss']\n","val_loss = history_dict['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","fig = plt.figure(figsize=(10, 6))\n","fig.tight_layout()\n","\n","plt.subplot(2, 1, 1)\n","# \"bo\" is for \"blue dot\"\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","# b is for \"solid blue line\"\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","# plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(epochs, acc, 'r', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WzJZCo-cf-Jf"},"source":["In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."]},{"cell_type":"markdown","metadata":{"id":"Rtn7jewb6dg4"},"source":["## Export for inference\n","\n","Now you just save your fine-tuned model for later use."]},{"cell_type":"code","metadata":{"id":"ShcvqJAgVera"},"source":["dataset_name = 'RANDOM'\n","saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n","\n","classifier_model.save(saved_model_path, include_optimizer=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PbI25bS1vD7s"},"source":["Let's reload the model, so you can try it side by side with the model that is still in memory."]},{"cell_type":"code","metadata":{"id":"gUEWVskZjEF0"},"source":["reloaded_model = tf.saved_model.load(saved_model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oyTappHTvNCz"},"source":["Here you can test your model on any sentence you want, just add to the examples variable below."]},{"cell_type":"code","metadata":{"id":"VBWzH6exlCPS"},"source":["def print_my_examples(inputs, results):\n","  result_for_printing = \\\n","    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n","                         for i in range(len(inputs))]\n","  print(*result_for_printing, sep='\\n')\n","  print()\n","!unzip 'abstracts_8.11.zip'\n","import pathlib\n","examples = []\n","\n","#put name of directory in place of test001\n","for path in pathlib.Path(\"abstracts_8.11\").iterdir():\n","    if path.is_file():\n","        current_file = open(path, \"r\")\n","        content = current_file.read()\n","        examples.append(content)\n","        current_file.close()\n","\n","reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n","original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n","\n","print('Results from the saved model:\\n')\n","print_my_examples(examples, reloaded_results)\n","#print('Results from the model in memory:')\n","#print_my_examples(examples, original_results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5AGTyWYGB_rg"},"source":["def print_my_examples(inputs, results):\n","  result_for_printing = \\\n","    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n","                         for i in range(len(inputs))]\n","  print(*result_for_printing, sep='\\n')\n","  print()\n","\n","\n","examples = [\n","    'Youngs Modulus',  # this is the same sentence tried earlier\n","    'Youngs modulus of Al',\n","    'Youngs Modulus of Al is 70 GPa',\n","    'Bulk Modulus', \n","    'Bulk modulus of Al',\n","    'Bulk Modulus of Al is 76 GPa',\n","]\n","\n","reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n","original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n","\n","print('Results from the saved model:')\n","print_my_examples(examples, reloaded_results)\n","print('Results from the model in memory:')\n","print_my_examples(examples, original_results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3cOmih754Y_M"},"source":["If you want to use your model on [TF Serving](https://www.tensorflow.org/tfx/guide/serving), remember that it will call your SavedModel through one of its named signatures. In Python, you can test them as follows:"]},{"cell_type":"code","metadata":{"id":"0FdVD3973S-O"},"source":["serving_results = reloaded_model \\\n","            .signatures['serving_default'](tf.constant(examples))\n","\n","serving_results = tf.sigmoid(serving_results['classifier'])\n","\n","print_my_examples(examples, serving_results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4gN1KwReLPN"},"source":["## Next steps\n","\n","As a next step, you can try [Solve GLUE tasks using BERT on a TPU tutorial](https://www.tensorflow.org/text/tutorials/bert_glue), which runs on a TPU and shows you how to work with multiple inputs."]}]}